## LM Studio configuration
# Model identifier to use with LM Studio (must be available in LM Studio)
LMSTUDIO_MODEL=google/gemma-3-12b

# Where LM Studio server listens on your machine
LMSTUDIO_HOST=localhost
LMSTUDIO_PORT=1234

# Directory on the host where the Fastify proxy SQLite DB will be stored
LMSTUDIO_SQLITE_HOST_DIR=./fastify-proxy/data

# Optional webhooks fired by the Fastify proxy after chat completions
LMSTUDIO_WEBHOOK_ON_CHAT_COMPLETE=https://example.com/webhook
LMSTUDIO_WEBHOOK_ON_CHAT_COMPLETE_HEADERS='{"Authorization":"Bearer abc123"}'

# Enable/disable SQLite logging in the Fastify proxy ("false" to turn off logging)
LMSTUDIO_SQLITE_LOGGING=true

# When true, store only a trimmed+hashed version of request/response bodies in SQLite
# to reduce exposure of sensitive content (GDPR-friendly logging)
LMSTUDIO_SQLITE_PRIVACY_TRIM=false

## Nginx configuration
NGINX_PORT=8080
NGINX_SSL_PORT=8443

# Nginx timeout configuration (should be slightly higher than LM Studio / bridge timeouts)
NGINX_PROXY_CONNECT_TIMEOUT=90
NGINX_PROXY_SEND_TIMEOUT=330
NGINX_PROXY_READ_TIMEOUT=330

## Ngrok configuration
NGROK_AUTHTOKEN=your_ngrok_auth_token_here
NGROK_REGION=us

## Authentication
AUTH_USERNAME=admin
AUTH_PASSWORD=secure_password_123

## Rate limiting
RATE_LIMIT=10r/s
RATE_BURST=20

## VLLM bridge (optional)
# Enable the optional VLLM bridge profile in docker-compose
VLLM_BRIDGE_ENABLED=true
VLLM_BRIDGE_PORT=8000

# VLLM bridge timeouts (seconds)
VLLM_BRIDGE_CHAT_TIMEOUT=300
VLLM_BRIDGE_MODELS_TIMEOUT=30

## SSL configuration (optional)
SSL_ENABLED=false
SSL_CERT_PATH=./certs/server.crt
SSL_KEY_PATH=./certs/server.key
